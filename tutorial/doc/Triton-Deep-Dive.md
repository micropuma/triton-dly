 # Triton Deep Dive
In this doc, we will dive into triton internals, to show how our hand-written triton-lang is lowered into ptx/cubin and finally launched by cuda driver. This doc is organized into following parts:  
* Overall framework 
* Source code structure
* Dive into jit compilation (Python)
* Pybind gluer
* MLIR Lowering passes (C++)

## Chapter1: Overall Framework
The [Triton](https://github.com/triton-lang/triton) codes nowadays is actually quite different from the original [paper](https://dl.acm.org/doi/10.1145/3315508.3329973). There are two major differences:  
1. Python DSL is introduced to make it easier for model developers. Pythonic is the trend nowadays in ML compiler world :key:.
2. Passes are all rewritten in MLIR, making it more extendible.  

So we can clearly see two different parts in this shift, a python wrapper for user end, and the actual code optimization and conversion realized in C++ with MLIR ecosystem, glued by [pybind](git@github.com:pybind/pybind11.git) mechanism.  

With this bluescript in mind, we now try to touch three important questions:  
1. What is the input?
2. What are temporaries and final codes during Triton compilation?
3. How are those compiled codes loaded by device driver?   

Let's explain these questions one by one.

> :question: What is the input?  
> Input of triton source code merely contains three parts: (1) triton kernel (2) kernel wrapper that can easily replace counterpart operator in certain ml models (3) data preparation and kernel launch. Below is a basic code sample:  
```python
@triton.jit  
def _kernel(...):
    pass

def kernel(...):
    _kernel[grid](...)  # SPMD programming model

# data preparation
A ...
B ...
C ...
# kernel call
D = kernel(A, B, C)
# correctness test and perf test
...
```

> :question: What are the temporaries and the final codes?  
> All temporaries are saved on disk, user can set TRITON_CACHE_DIR, or by default the dir path is ~/.triton/cache. Following figure shows files in the triton cache:  
> ![cache_file](../png/cache.png)  
Here is a simple descriptions for those files:   
`cuda_utils.so`contains cuda helper functions for correct cuda launching. `triton_launcher.so`helps launch cuda kernels. `.source` is user-written triton dsl, `.ttir` is a one-to-one mapping ir, `ttgir` is gpu aware ir, `llir` is llvm ir converted from `ttgir`. Final result is `.ptx`, a cuda assembly. `.json` contains all the metadata of this kernel, which can help jit compile and launch(hash this compiled kernel for future use).
  
> :question: How are ptx kernel launched by device driver?  
> Secrete lies in `.so` file. This file contains wrapper for `cuKernelLaunch` function、grid configurations and parameters, which enable python to dynamically load this .so with importlib, and finally launch ptx/cubin kernels with control of device driver.  

With these answers in mind, we can get a clearer roadmap of triton compiler. Then let's touch source code to make it more solid. First let's have a overall understanding of the codebase structure.

## Chapter2: Source code structure  
Triton Compiler project contains following directories:  
* `/python/triton/tools/compile.py` – The top-level compiler tool.
* `/python/triton/runtime/jit.py` – The runtime utility, which includes kernel compilation cache, source data management, driver interaction, and kernel scheduling. :key:
* `/python/triton/compiler/code_generator.py` – Mainly handles the AST generated by the DSL, translating it into MLIR IR (i.e., Triton IR).
* `/third_party/nvidia/backend/compiler.py` – The compilation pipeline for specific hardware vendors, such as NVIDIA’s CUDA flow. Typically involves the transformation of TritonGPU IR and code generation for PTX and beyond. :key:
* `triton/python/src/passes.cc` – Glue layer(pybind) that organizes the various passes. :key:
* `*/lib` - cpp codebase, contains mlir source code for ir optimization, analysis and conversion. :key:
* `*/include` - header file for lib. :key:  

Let's first dive into python layer, to grasp how JIT compilation works in triton.

## Chapter3: JIT compile  
This part, we focus on `python/triton/runtime/` repository, mainly `jit.py` , `compile.py` and `build.py`. First let's see a flowgraph to have a general understanding of triton jit compilation:  
![jit](../png/jit.png)

We can divide the whole jit compile workflow into three parts, and readers can refer to source code each part for further understanding: 
* Jit Interface: this part mainly deal with user-written triton-lang kernel and kernel launch. Jit main function is [here](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L885-L937), and there are two main classes that we should pay attention with, JitFunction and its parent class KernelInterface. In KernelInterface, we should focus on [`__getitem__()`](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L411-L421)function, which is the entry of `kernel[grid]()`. And in JitFunction, [`run()`](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L693-L744) function is quite essential.
* Compilation: this part works when our kernel is firstly compiled, no previous cached kernel matched. This part is mainly written in mlir c++, turn to [`make_ttir()`](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L229-L243),[`make_ttgir()`](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L245-L323),[`make_llir()`](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L341-L411),[`make_ptx()`](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L413-L443) and [`make_cubin()`](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L435-L494) for further dive. This pipeline is the keypoint in next chapter.   
    > This pipeline is vendor specific. For Nvidia GPU, refer to [cuda backend compiler](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py).
* Runtime & Driver: this part first turn compiled cubin into `.so` lib, refer [here](https://github.com/triton-lang/triton/blob/main/python/triton/compiler/compiler.py#L412-L441). Then use [`init_handle()`](https://github.com/triton-lang/triton/blob/main/python/triton/compiler/compiler.py#L443-L472) to init a cuda stream and all metadata. Finally use [`launcher_cls`](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/driver.py#L723-L726) to evoke a [`CudaLuancher`](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/driver.py#L677-L718) class and finally launch a cuda kernel.

Hoping by following the flow graph and descriptions of each key part of JIT compilation, readers can get a clear picture of the whole python codebase and jit flow. Then comes the most exciting part to MLIR compilers, the actual code transformation and optimizations, proposing many interesting and advanced gpu compilation techniques such as: [triton layout](https://www.lei.chat/posts/triton-linear-layout-concept/), coalesce opt, tensor core opt, etc. 

## Chapter4: MLIR Lowering Passes
This part, we enter the core optimization pipeline of Triton. The whole lowering pipeline is: triton-lang -> ttir -> ttgir -> llir -> ptx -> cubin. Refer to [triton pipeline](./pipeline) for detailed code transformation of a basic vector add triton-lang operator.  

### TTIR
```python
@staticmethod
def make_ttir(mod, metadata, opt, capability):     # triton ir，主要描述上层计算行为
    import pdb
    pdb.set_trace()
    pm = ir.pass_manager(mod.context)
    pm.enable_debug()
    passes.common.add_inliner(pm)
    passes.ttir.add_rewrite_tensor_pointer(pm)
    if capability // 10 < 9:
        passes.ttir.add_rewrite_tensor_descriptor_to_pointer(pm)
    passes.common.add_canonicalizer(pm)
    passes.ttir.add_combine(pm)
    passes.ttir.add_reorder_broadcast(pm)
    passes.common.add_cse(pm)
    passes.common.add_symbol_dce(pm)
    passes.ttir.add_loop_unroll(pm)
    pm.run(mod)
    return mod
```

In this doc, we only focus on Triton-specific optimizations, that are [`RewriteTensorPointer`](https://github.com/triton-lang/triton/blob/main/lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp), [`Combine`](https://github.com/triton-lang/triton/blob/main/lib/Dialect/Triton/Transforms/Combine.cpp) and [`ReorderBroadcast`](https://github.com/triton-lang/triton/blob/main/lib/Dialect/Triton/Transforms/ReorderBroadcast.cpp) three passes. This part highly refer to [OpenAI Triton 源码走读[transforms in ttir]](https://tfruan2000.github.io/posts/triton-source-code-1/).

#### RewriteTensorPointer
The figure below shows all details on how this pass do to `tl.make_block_ptr` and `tl.advance` operations.  
![tensor-rewrite pass](../png/tensor-rewrite.png)

Refer to [Before-pass-example](../Triton-101/DeepDive/ttir/test/test_rewrite.mlir) and [After-pass-example](../Triton-101/DeepDive/ttir/test/result1.mlir) to see detailed effects of this pass.

> A tip to help debug pattern rewrite in mlir: use `-debug-only=greedy-rewriter`. Refer to [Pattern Rewrite](https://mlir.llvm.org/docs/PatternRewriter/) for further details.

#### Combine
The table below shows all combination rules that triton applied:  

| Pattern Name                        | Match Rule                                                                                  | Rewrite Result                                             | Optimization Purpose                                                                                   |
|-------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|
| CombineDotAddIPattern               | `AddIOp(d, DotOp(a, b, c=0))`                                                               | `DotOp(a, b, d)`                                           | Merge dot with zero-init + add to eliminate redundant `add`.                                           |
| CombineDotAddFPattern               | `AddFOp(d, DotOp(a, b, c=0, maxNumImpreciseAcc=0))`                                         | `DotOp(a, b, d)`                                           | Same as above, but for floating-point add, restricted to `maxNumImpreciseAcc == 0`.                    |
| CombineDotAddIRevPattern            | `AddIOp(DotOp(a, b, c=0), d)`                                                               | `DotOp(a, b, d)`                                           | Same as `CombineDotAddIPattern`, but with `dot` on the left-hand side.                                |
| CombineDotAddFRevPattern            | `AddFOp(DotOp(a, b, c=0, maxNumImpreciseAcc=0), d)`                                         | `DotOp(a, b, d)`                                           | Same as `CombineDotAddFPattern`, but with `dot` on the left-hand side.                                |
| CombineAddPtrPattern                | `addptr(addptr(ptr, idx0), idx1)`                                                           | `addptr(ptr, AddIOp(idx0, idx1))`                          | Merge multi-level pointer offsets to avoid nested `addptr`; preserve optional attributes (div/cont/const). |
| CombineSelectMaskedLoadPattern      | `select(cond, load(ptrs, splat(cond), ?), other)`                                           | `load(ptrs, splat(cond), other)`                           | Merge `select`-wrapped masked load into a more concise `load`.                                        |
| CombineBroadcastMulReducePattern    | `reduce(sum, broadcast(x[:, :, None]) * broadcast(y[None, :, :]))`                          | `dot(x, y)`                                                | Recognize matrix multiplication pattern (broadcast-mul-reduce) and replace with efficient `dot`.       |
| CombineReshapeReducePatterns        | `reshape(tensor)` (1D, `allowReorder=false`, user is reduce/histogram)                      | set `allowReorder=true`                                    | Enable element reordering for 1D tensor reshape in reduction/histogram cases, improving optimization. |
| RankedReduceDescriptorLoads         | `reshape(descriptor_load(...))` with rank-reducing reshape                                  | absorb reshape into `descriptor_load` and modify result type | Eliminate meaningless reshape by folding it into `descriptor_load`.                                   |


#### ReorderBroadcast
The table below lists all broadcast + elementwise reorder rules that triton applied:

| Pattern Name | Original Form | Reordered Form | Conditions | Purpose |
|--------------|--------------|----------------|------------|---------|
| MoveSplatAfterElementwisePattern | `elementwise(splat(a), splat(b), ...)` | `splat(elementwise(a, b, ...))` | - All operands are `SplatOp` or constant splats.<br>- Operation is **elementwise** and has no side effects. | Compute on scalars first, then splat once → avoids redundant tensor elementwise ops. |
| MoveBroadcastAfterElementwisePattern | `elementwise(broadcast(a), splat(b), ...)` | `broadcast(elementwise(a, b, ...))` | - At most one broadcast operand.<br>- All broadcasts must expand to the **same shape**.<br>- Operation is **elementwise** and has no side effects. | Compute on the smaller source tensor, then broadcast once → reduces duplicated computation. |
| Canonicalization (built-in) | e.g., `broadcast(broadcast(a))`, `expand_dims(expand_dims(a))` | Simplified canonical form | Provided by `BroadcastOp` and `ExpandDimsOp`. | Normalizes IR to expose more rewrite opportunities and remove redundant ops. |  

These three passes are all hardware-agnostic optimizations, to make further analysis and transformation more efficient. 

### TTIR -> TTGIR
This part highly refer to [OpenAI Triton 源码走读[ttir-2-ttgir]](https://tfruan2000.github.io/posts/triton-source-code-2/), [Triton Linear Layout: Concept](https://www.lei.chat/posts/triton-linear-layout-concept/) and [Triton Axis Analysis](https://zhuanlan.zhihu.com/p/687394750). Let's first see what this pass make changes to IR.

`IR before conversion`
```cpp
module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py":32:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py":32:0), %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py":32:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py":32:0)) attributes {noinline = false} { ... }
}
```

`IR after conversion`
```cpp
#blocked = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#loc = loc("/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py":32:0)
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:86", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py":32:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py":32:0), %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py":32:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py":32:0)) attributes {noinline = false} { ... }
}
```  

The main difference is:  
```cpp
#blocked = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#loc = loc("/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py":32:0)
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:86", "ttg.threads-per-warp" = 32 : i32}
```

In conclusion, the main difference after ttir->ttgir conversion is the layout attribute attached to IR tensors, which defines how the data is parallelized and processed by threads. This layout attribute is propagated during the lowering process. In this doc, we are more interested in how TTIR is converted to TTGIR, and how Layout attrs are attatched and transformed. Readers should refer to [TritonGPUAttr](https://github.com/triton-lang/triton/blob/main/include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td#L62-L704) and [Triton Layout doc](./Triton-Layout.md)for prior knowledge on triton layout.




### TTGIR  
Readers can refer to [make_ttgir](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L245-L323) to view pass pipelines in TTGIR dialect.   

In this part, triton do all critical GPU-specific optimizations, with the help fo Axis analysis. Let's first dive into the design of Axis analysis, combined with previous layout information, which clearly shows how tensors are stored and distribute within threads.   

#### Axis Analysis
Axis Analysis is like other standard analysis pass, gather detailed information to help guide further code transformation. This part highly refer to [OpenAI Triton: Dive into Axis and Coalesce](https://www.zhihu.com/search?type=content&q=triton%20axis%20analysis) blog. To fully understand the detailed implementation behind, we need to understand an important concept: **Dataflow Analysis** and how it is implemented in `Triton` with the help of mlir infrastructure.  

Below is a Flow graph shows the whole dataflow process:  

![AxisInfo Analysis](../png/dataflow.png)

> Refer to [ModuleAxisInfoAnalysis](https://github.com/triton-lang/triton/blob/main/include/triton/Analysis/AxisInfo.h#L191-L270) for detailed implementation of ModuleAxisInfoAnalysis, pay attention to **its class constructure**, [**initialize()**](https://github.com/triton-lang/triton/blob/main/lib/Analysis/AxisInfo.cpp#L1311-L1348). Refer to [AxisInfoAnalysis](https://github.com/triton-lang/triton/blob/main/lib/Analysis/AxisInfo.cpp#L137-L181) for AxisInfoAnalysis, where [**visitOperation()**](https://github.com/triton-lang/triton/blob/main/lib/Analysis/AxisInfo.cpp#L1040-L1075) is essential. Also refer to []() for AxisInfo to see lattice define in dataflow analysis.

Three new concepts are introduced in this phase:  
* Divisibility
* Contiguity
* Constancy  

`Divisibility`
```shell
// For example, the 2D array
//
//   [[10, 11, 12, 13, 18, 19, 20, 21],
//    [20, 21, 22, 23, 28, 29, 30, 31]]
//
// has contiguity [1, 4], and
//
//   [[12, 16, 20, 24],
//    [13, 17, 21, 25],
//    [14, 18, 22, 26],
//    [15, 19, 23, 27],
//    [18, 22, 26, 30],
//    [19, 23, 27, 31]]
//
// has contiguity [2, 1].
```

`Contiguity`  
```shell
// For example,
//
//   [[10, 11, 12, 13, 18, 19, 20, 21],
//    [20, 21, 22, 23, 28, 29, 30, 31]]
//
//  has divisibility [1, 2], and
//
//    [[12, 16, 20, 24],
//     [13, 17, 21, 25],
//     [14, 18, 22, 26],
//     [15, 19, 23, 27]]
//
// has divisibility [4, 1].
//
// On the other hand,
//
//   [0, 1, 2, 0, 4, 5, 6, 7]
//
// has divisibility 1 because its contiguity is 1.
```

`Constancy`
```shell
// For example
//
//   [[8, 8, 8, 8, 12, 12, 12, 12],
//    [16, 16, 16, 16, 20, 20, 20, 20]]
//
// has constancy [1, 4].
```

#### Coalesce Pass

## Chapter5: Python Binding Layer
refer to [python/src/passes.cpp](https://github.com/triton-lang/triton/blob/main/python/src/passes.cc#L20-L128) and [python/src/passes.h](https://github.com/triton-lang/triton/blob/main/python/src/passes.h#L1-L43)

## References
1. [Triton Internal Talk](https://github.com/gpu-mode/lectures/blob/main/lecture_029/presentation.pdf/) 
2. [Triton Fusing Talk](https://github.com/gpu-mode/lectures/tree/main/lecture_018)
3. [A Practioner's Guide to Triton](https://github.com/gpu-mode/lectures/blob/main/lecture_014/A_Practitioners_Guide_to_Triton.ipynb) 
4. [Deep Dive into Triton 1&2&3](https://www.kapilsharma.dev/posts/deep-dive-into-triton-internals/)
5. [Official Hacking for Triton](https://github.com/kapilsh/triton/tree/14025786d108596cfd99700caa4f438938c2ceba?tab=readme-ov-file#tips-for-hacking) 
6. [Triton Developer Guide](https://www.lei.chat/posts/triton-compiler-development-tips/)
7. [Triton Linear Layout: Concept](https://www.lei.chat/posts/triton-linear-layout-concept/)
8. [OpenAI Triton: Why layout is important](http://zhuanlan.zhihu.com/p/672720213) 
9. [OpenAI Triton 源码走读[transforms in ttir]](https://tfruan2000.github.io/posts/triton-source-code-1/)
10. [OpenAI Triton 源码走读[ttir-2-ttgir]](https://tfruan2000.github.io/posts/triton-source-code-2/)
11. [Triton Axis Analysis](https://zhuanlan.zhihu.com/p/687394750) 